{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to build a neural network for the classification of the MNIST dataset. This neural network will comprise of: an output layer with 10 nodes, a hidden layer of 128 nodes, and an input layer with 784 nodes corresponding to the image pixels. The specific structure of the neural network is outlined below, where $X$ represents the input, $A^{[0]}$ denotes the first layer, $Z^{[1]}$ signifies the unactivated layer 1, $A^{[1]}$ stands for the activated layer 1, and so forth. The weights and biases are represented by $W$ and $b$ respectively:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$A^{[0]}=X$\n",
    "\n",
    "$Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$\n",
    "\n",
    "$A^{[1]}=\\text{ReLU}(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$\n",
    "\n",
    "$A^{[2]}=\\text{softmax}(Z^{[2]})$\n",
    "\n",
    "$Loss=\\text{cross-entropy-loss}(A^{[2]})$\n",
    "</div>\n",
    "\n",
    "You have the flexibility to create any function within or outside the class, allowing you to modify parameters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mnist \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mmnist\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the activation function(ReLU) and softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the important part.\n",
    "\n",
    "In this, you will implement the NN class, which will be the model you will be using to train data on and later use it to predict.\n",
    "\n",
    "You have been given the init function, you have to implement all the other functions yourself, in any way you like ... you may even skip some of them if you don't need them in the final implementation of the class.\n",
    "\n",
    "The description of each function has been given in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # initialized basic stats of NN\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        #initialized weights and biases\n",
    "        self.W1=np.random.randn(hidden_size, input_size)*0.01\n",
    "        self.b1= np.zeros((hidden_size, 1))\n",
    "        self.W2=np.random.randn(output_size, hidden_size)*0.01\n",
    "        self.b2=np.zeros((output_size, 1))\n",
    "\n",
    "        #initialized activations and gradients\n",
    "        self.AO=None\n",
    "        self.Z1=None\n",
    "        self.A1=None\n",
    "        self.Z2=None\n",
    "        self.A2=None\n",
    "        self.dW2=None\n",
    "        self.db2=None\n",
    "        self.dW1=None\n",
    "        self.db1=None\n",
    "\n",
    "    # do the forward pass and evaluate the values of A0, Z1, A1, Z2, A2\n",
    "    def forward_propagation(self, X):\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = softmax(self.Z2)\n",
    "        self.AO = self.A2\n",
    "        return self.AO\n",
    "    \n",
    "    # convert the input y, into a one hot encoded array.\n",
    "    '''\n",
    "    one hot encoding is:\n",
    "    you have an array with values [2, 5, 6] and you know the max value can be 8, then one hot encoded array will be:\n",
    "    [[0,0,1,0,0,0,0,0,0], [0,0,0,0,0,1,0,0,0], [0,0,0,0,0,0,1,0,0]]\n",
    "    Note that the index 2, 5, 6 have values 1 and all others have values 0\n",
    "    '''\n",
    "    def one_hot(self, y):\n",
    "        one_hot_y = np.zeros((y.size, self.output_size))\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "        return one_hot_y.T\n",
    "\n",
    "    # calculate the derivative of the loss function with respect to W2, b2, W1, b1 in dW2, db2, dW1, db1 respectively\n",
    "    def backward_propagation(self, X, y):\n",
    "        one_hot_y = self.one_hot(y)\n",
    "        m = X.shape[1]\n",
    "\n",
    "        self.dZ2 = self.A2 - one_hot_y\n",
    "        self.dW2 = (1 / m) * np.dot(self.dZ2, self.A1.T)\n",
    "        self.db2 = (1 / m) * np.sum(self.dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(self.W2.T, self.dZ2)\n",
    "        dZ1 = dA1 * (self.Z1 > 0)\n",
    "        self.dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "        self.db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # update the parameters W1, W2, b1, b2\n",
    "    def update_params(self):\n",
    "        self.W1 -= self.learning_rate * self.dW1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        self.W2 -= self.learning_rate * self.dW2\n",
    "        self.b2 -= self.learning_rate * self.db2\n",
    "    \n",
    "    # get the predictions for the dataset\n",
    "    def get_predictions(self):\n",
    "        return np.argmax(self.A2, axis=0)\n",
    "\n",
    "    # get the accuracy of the model on the dataset\n",
    "    def get_accuracy(self, X, y):\n",
    "        predictions = self.get_predictions()\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "    # run gradient descent on the model to get the values of the parameters\n",
    "    def gradient_descent(self, X, y, iters=1000):\n",
    "        for i in range(iters):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, y)\n",
    "            self.update_params()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {self.cross_entropy_loss(X, y)}, Accuracy: {self.get_accuracy(X, y)}\")\n",
    "    \n",
    "    # evaluate loss using cross-entropy-loss formula.\n",
    "    def cross_entropy_loss(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        p = self.forward_propagation(X)\n",
    "        log_likelihood = -np.log(p[y, range(m)])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    # Let me help a bit hehe :)\n",
    "    def show_predictions(self, X, y, num_samples=10):\n",
    "        random_indices = np.random.randint(0, X.shape[0], size=num_samples)\n",
    "\n",
    "        for index in random_indices:\n",
    "            sample_image = X[index, :].reshape((28, 28))\n",
    "            plt.imshow(sample_image, cmap='gray')\n",
    "            plt.title(f\"Actual: {y[index]}, Predicted: {self.get_predictions()[index]}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are splitting the dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (X_train, Y_train), (X_test, Y_test) \u001b[38;5;241m=\u001b[39m \u001b[43mmnist\u001b[49m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      3\u001b[0m miu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X_train, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_train, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "miu = np.mean(X_train, axis=(0, 1), keepdims=True)\n",
    "stds = np.std(X_train, axis=(0, 1), keepdims=True)\n",
    "\n",
    "mius = np.mean(X_test, axis=(0, 1), keepdims=True)\n",
    "stdse = np.std(X_test, axis=(0, 1), keepdims=True)\n",
    "\n",
    "X_normal_train = (X_train - miu) / (stds + 1e-7)\n",
    "X_normal_test = (X_test - mius) / (stdse + 1e-7)\n",
    "\n",
    "X_normal_train = X_normal_train.reshape((60000, -1)).T\n",
    "X_normal_test = X_normal_test.reshape((10000, -1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will train the model on X_normal_train and Y_train dataset\n",
    "\n",
    "Then print the accuracy of model on X_normal_test and Y_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_normal_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m nn \u001b[38;5;241m=\u001b[39m NN(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the neural network\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nn\u001b[38;5;241m.\u001b[39mgradient_descent(\u001b[43mX_normal_train\u001b[49m, Y_train, iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Evaluate the neural network\u001b[39;00m\n\u001b[0;32m     10\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mget_accuracy(X_normal_test, Y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_normal_train' is not defined"
     ]
    }
   ],
   "source": [
    "# All the best :)\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = NN(input_size=784, hidden_size=128, output_size=10, learning_rate=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "nn.gradient_descent(X_normal_train, Y_train, iters=1000)\n",
    "\n",
    "# Evaluate the neural network\n",
    "test_accuracy = nn.get_accuracy(X_normal_test, Y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
